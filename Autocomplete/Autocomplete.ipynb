{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\14694\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"en_US.twitter.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "\n",
    "    Args:\n",
    "        data (str): input data\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    return tokenized_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    \n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(100)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenize_sentences):\n",
    "    word_counts = {}\n",
    "    \n",
    "    for sentence in tokenize_sentences:\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \n",
    "    closed_vocab = []\n",
    "    \n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab\n",
    "\n",
    "\n",
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token = '<unk>'):\n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in vocabulary:\n",
    "                replaced_sentence.append(word)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        \n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    return replaced_tokenized_sentences\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold, unknown_token= \"unk\"):\n",
    "    \n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    \n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n",
    "    \n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['some', 'of', 'the', 'highlights', 'from', 'this', 'report', 'include', 'the', 'popularity', 'of', 'games', 'by', 'platform', 'and', 'stats', 'on', 'the', 'top', 'publishers']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['i', 'know', 'it', \"'s\", 'not', 'their', 'fault', 'personally', ',', 'but', 'where', \"'s\", 'the', 'unk', 'apple', 'store', 'employee', 'so', 'i', 'can', 'punch', 'them', 'in', 'the', 'face', 'real', 'quick', '?']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['some', 'of', 'the', 'highlights', 'from', 'this', 'report', 'include', 'popularity', 'games']\n",
      "\n",
      "Size of vocabulary: 14794\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
